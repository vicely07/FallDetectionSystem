
#---------------------------------------Creating attention model-----------------------------------------------------------------
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np
from keras import regularizers, initializers, constraints

class AttentionWithContext(Layer):

    """

        Attention operation, with a context/query vector, for temporal data.

        Supports Masking.

        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]

        "Hierarchical Attention Networks for Document Classification"

        by using a context vector to assist the attention

        # Input shape

            3D tensor with shape: `(samples, steps, features)`.

        # Output shape

            2D tensor with shape: `(samples, features)`.

        :param kwargs:

        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.

        The dimensions are inferred based on the output shape of the RNN.

        Example:

            model.add(LSTM(64, return_sequences=True))

            model.add(AttentionWithContext())

        """



    def __init__(self, init='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None,  **kwargs):

        self.supports_masking = True

        self.init = initializers.get(init)

        self.kernel_initializer = initializers.get('glorot_uniform')



        self.kernel_regularizer = regularizers.get(kernel_regularizer)

        self.bias_regularizer = regularizers.get(bias_regularizer)



        self.kernel_constraint = constraints.get(kernel_constraint)

        self.bias_constraint = constraints.get(bias_constraint)



        super(AttentionWithContext, self).__init__(**kwargs)



    def build(self, input_shape):

        self.kernel = self.add_weight((input_shape[-1], 1),

                                 initializer=self.kernel_initializer,

                                 name='{}_W'.format(self.name),

                                 regularizer=self.kernel_regularizer,

                                 constraint=self.kernel_constraint)

        self.b = self.add_weight((input_shape[1],),

                                 initializer='zero',

                                 name='{}_b'.format(self.name),

                                 regularizer=self.bias_regularizer,

                                 constraint=self.bias_constraint)



        self.u = self.add_weight((input_shape[1],),

                                 initializer=self.kernel_initializer,

                                 name='{}_u'.format(self.name),

                                 regularizer=self.kernel_regularizer,

                                 constraint=self.kernel_constraint)

        self.built = True



    def compute_mask(self, input, mask):

        return None



    def call(self, x, mask=None):

        # (x, 40, 300) x (300, 1)

        multData =  K.dot(x, self.kernel) # (x, 40, 1)

        multData = K.squeeze(multData, -1) # (x, 40)

        multData = multData + self.b # (x, 40) + (40,)



        multData = K.tanh(multData) # (x, 40)



        multData = multData * self.u # (x, 40) * (40, 1) => (x, 1)

        multData = K.exp(multData) # (X, 1)



        # apply mask after the exp. will be re-normalized next

        if mask is not None:

            mask = K.cast(mask, K.floatx()) #(x, 40)

            multData = mask*multData #(x, 40) * (x, 40, )



        # in some cases especially in the early stages of training the sum may be almost zero

        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.

        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())

        multData /= K.cast(K.sum(multData, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        multData = K.expand_dims(multData)

        weighted_input = x * multData

        return K.sum(weighted_input, axis=1)





    def compute_output_shape(self, input_shape):

        return (input_shape[0], input_shape[-1],)
#--------------------------------------------unused layer------------------------------------------------------------------------
'''
from keras import backend as K
from keras.engine import InputSpec
from keras.layers import LSTM, activations, Wrapper, Recurrent, recurrent

class AttentionLSTM(Wrapper):
    """
    This wrapper will provide an attention layer to a recurrent layer. 
     
    # Arguments:
        layer: `Recurrent` instance with consume_less='gpu' or 'mem'
     
    # Examples:
     
    ```python
    model = Sequential()
    model.add(LSTM(10, return_sequences=True), batch_input_shape=(4, 5, 10))
    model.add(TFAttentionRNNWrapper(LSTM(10, return_sequences=True, consume_less='gpu')))
    model.add(Dense(5))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop') 
    ```
     
    # References
    - [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449)
     
     
    """
    def __init__(self, layer, **kwargs):
        #assert isinstance(layer, Recurrent)
        if layer.get_config()['consume_less']=='cpu':
            raise Exception("AttentionLSTMWrapper doesn't support RNN's with consume_less='cpu'")
        self.supports_masking = True
        super(AttentionLSTM, self).__init__(layer, **kwargs)
 
    def build(self, input_shape):
        assert len(input_shape) >= 3
        self.input_spec = [InputSpec(shape=input_shape)]
        nb_samples, nb_time, input_dim = input_shape
 
        if not self.layer.built:
            self.layer.build(input_shape)
            self.layer.built = True
 
        super(AttentionLSTM, self).build()
         
        self.W1 = self.layer.init((input_dim, input_dim, 1, 1), name='{}_W1'.format(self.name))
        self.W2 = self.layer.init((self.layer.output_dim, input_dim), name='{}_W2'.format(self.name))
        self.b2 = K.zeros((input_dim,), name='{}_b2'.format(self.name))
        self.W3 = self.layer.init((input_dim*2, input_dim), name='{}_W3'.format(self.name))
        self.b3 = K.zeros((input_dim,), name='{}_b3'.format(self.name))
        self.V = self.layer.init((input_dim,), name='{}_V'.format(self.name))
 
        self.trainable_weights = [self.W1, self.W2, self.W3, self.V, self.b2, self.b3]
 
    def get_output_shape_for(self, input_shape):
        return self.layer.get_output_shape_for(input_shape)
 
    def step(self, x, states):
        # This is based on [tensorflows implementation](https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/python/ops/seq2seq.py#L506).
        # First, we calculate new attention masks:
        #   attn = softmax(V^T * tanh(W2 * X +b2 + W1 * h))
        # and we make the input as a concatenation of the input and weighted inputs which is then
        # transformed back to the shape x of using W3
        #   x = W3*(x+X*attn)+b3
        # Then, we run the cell on a combination of the input and previous attention masks:
        #   h, state = cell(x, h).
         
        nb_samples, nb_time, input_dim = self.input_spec[0].shape
        h = states[0]
        X = states[-1]
        xW1 = states[-2]
         
        Xr = K.reshape(X,(-1,nb_time,1,input_dim))
        hW2 = K.dot(h,self.W2)+self.b2
        hW2 = K.reshape(hW2,(-1,1,1,input_dim)) 
        u = K.tanh(xW1+hW2)
        a = K.sum(self.V*u,[2,3])
        a = K.softmax(a)
        a = K.reshape(a,(-1, nb_time, 1, 1))
         
        # Weight attention vector by attention
        Xa = K.sum(a*Xr,[1,2])
        Xa = K.reshape(Xa,(-1,input_dim))
         
        # Merge input and attention weighted inputs into one vector of the right size.
        x = K.dot(K.concatenate([x,Xa],1),self.W3)+self.b3    
         
        h, new_states = self.layer.step(x, states)
        return h, new_states
 
    def get_constants(self, x):
        constants = self.layer.get_constants(x)
         
        # Calculate K.dot(x, W2) only once per sequence by making it a constant
        nb_samples, nb_time, input_dim = self.input_spec[0].shape
        Xr = K.reshape(x,(-1,nb_time,input_dim,1))
        Xrt = K.permute_dimensions(Xr, (0, 2, 1, 3))
        xW1t = K.conv2d(Xrt,self.W1,border_mode='same')     
        xW1 = K.permute_dimensions(xW1t, (0, 2, 3, 1))
        constants.append(xW1)
         
        # we need to supply the full sequence of inputs to step (as the attention_vector)
        constants.append(x)
         
        return constants
 
    def call(self, x, mask=None):
        # input shape: (nb_samples, time (padded with zeros), input_dim)
        input_shape = self.input_spec[0].shape
        if K._BACKEND == 'tensorflow':
            if not input_shape[1]:
                raise Exception('When using TensorFlow, you should define '
                                'explicitly the number of timesteps of '
                                'your sequences.\n'
                                'If your first layer is an Embedding, '
                                'make sure to pass it an "input_length" '
                                'argument. Otherwise, make sure '
                                'the first layer has '
                                'an "input_shape" or "batch_input_shape" '
                                'argument, including the time axis. '
                                'Found input shape at layer ' + self.name +
                                ': ' + str(input_shape))
 
        if self.layer.stateful:
            initial_states = self.layer.states
        else:
            initial_states = self.layer.get_initial_states(x)
        constants = self.get_constants(x)
        preprocessed_input = self.layer.preprocess_input(x)
         
 
        last_output, outputs, states = K.rnn(self.step, preprocessed_input,
                                             initial_states,
                                             go_backwards=self.layer.go_backwards,
                                             mask=mask,
                                             constants=constants,
                                             unroll=self.layer.unroll,
                                             input_length=input_shape[1])
        if self.layer.stateful:
            self.updates = []
            for i in range(len(states)):
                self.updates.append((self.layer.states[i], states[i]))
 
        if self.layer.return_sequences:
            return outputs
        else:
            return last_output
'''
#---------------------------------------3d CNN model--------------------------------------------------------------
from keras.models import Sequential

from keras.utils import multi_gpu_model

from keras.layers import Dense, TimeDistributed, Flatten, Dropout, LSTM

from keras.layers.convolutional import Convolution3D, MaxPooling3D

from keras import optimizers

import numpy as np

# Define model

model = Sequential()

from numpy import array

input_shape=(1, 16, 112, 112, 3) # l, h, w, c



model.add(TimeDistributed(Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same', name='conv1'), input_shape=input_shape))

model.add(TimeDistributed(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1')))

#model.add(TimeDistributed(Dropout(0.5)))


# 2nd layer group



model.add(TimeDistributed(Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same', name='conv2')))

model.add(TimeDistributed(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool2')))

#model.add(TimeDistributed(Dropout(0.5)))


# 3rd layer group



model.add(TimeDistributed(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3a')))

model.add(TimeDistributed(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3b')))

model.add(TimeDistributed(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3')))

#model.add(TimeDistributed(Dropout(0.5)))

# 4th layer group



model.add(TimeDistributed(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv4a')))

model.add(TimeDistributed(Convolution3D(512, 3, 3, 3, activation='relu',border_mode='same', name='conv4b')))

model.add(TimeDistributed(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4')))

#model.add(TimeDistributed(Dropout(0.5)))

# 5th layer group



model.add(TimeDistributed(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5a')))

model.add(TimeDistributed(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5b')))

model.add(TimeDistributed(MaxPooling3D(pool_size=(2, 1, 1), strides=(2, 1, 1), border_mode='valid', name='pool5')))

model.add(TimeDistributed(Flatten()))
model.add(TimeDistributed(Dense(4096, activation='relu', name='fc6')))
model.add(TimeDistributed(Dropout(.5)))
model.add(TimeDistributed(Dense(4096, activation='relu', name='fc7')))
model.add(TimeDistributed(Dropout(0.5)))
model.add(LSTM(24, return_sequences=True)) #consume_less='gpu'))


model.add(Dense(487, activation='softmax'))


#----------------------------------------------------------Load parameter-------------------------------------------------------
import caffe_pb2 as caffe
import numpy as np

p = caffe.NetParameter()
p.ParseFromString(
    open('conv3d_deepnetA_sport1m_iter_1900000', 'rb').read()
)

def rot90(W):
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            for k in range(W.shape[2]):
                W[i, j, k] = np.rot90(W[i, j, k], 2)
    return W

params = []
conv_layers_indx = [1, 4, 7, 9, 12, 14, 17, 19]

for i in conv_layers_indx:
    layer = p.layers[i]
    weights_b = np.array(layer.blobs[1].data, dtype=np.float32)
    weights_p = np.array(layer.blobs[0].data, dtype=np.float32).reshape(
        layer.blobs[0].num, layer.blobs[0].channels, layer.blobs[0].length,
        layer.blobs[0].height, layer.blobs[0].width
    )
    weights_p = np.transpose(weights_p, (2, 3, 4, 1, 0))
    weights_p = rot90(weights_p)
    params.append([weights_p, weights_b])


model_layers_indx = [0, 2, 4, 5, 7, 8, 10, 11]  
for i, j in zip(model_layers_indx, range(11)):
    model.layers[i].set_weights(params[j])

import h5py

model.save_weights('sports1M_weights.h5', overwrite=True)
json_string = model.to_json()
with open('sports1M_model.json', 'w') as f:
    f.write(json_string)
#-------------------------------------------------------Test-------------------------------------------------------------------------

from keras.models import model_from_json
#------------------------------------------------Add Attention LSTM layer---------------------------------------------------------

model = model_from_json(open('sports1M_model.json', 'r').read())
model.load_weights('sports1M_weights.h5')
model.add(AttentionWithContext())

model.add(Dense(487, activation='softmax'))
#sgd = optimizers.SGD(lr=0.003, decay=150000, momentum=0.5, nesterov=True)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#parallel_model = multi_gpu_model(model, gpus=2)
#parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='adam')

model.summary()

#-------------------------------------------------------------Test set--------------------------------------------------------------------
with open('labels.txt', 'r') as f:
    labels = [line.strip() for line in f.readlines()]
print('Total labels: {}'.format(len(labels)))


import cv2
import numpy as np

cap = cv2.VideoCapture('dM06AMFLsrc.mp4')

vid = []
while True:
    ret, img = cap.read()
    if not ret:
        break
    vid.append(cv2.resize(img, (171, 128)))
vid = np.array(vid, dtype=np.float32)

X = (vid[2000:2016, 8:120, 30:142, :].transpose((3, 0, 1, 2)))
X = np.array([X])
print(X[0].shape)
print(X.shape)
#output = np.append([1], X.shape)
#output = np.append([-1], output)

#output = X[0].reshape((-1, 1, 16, 112, 112, 3))
#print(output.shape[0])

output = X.reshape((-1, 1, 16, 112, 112, 3))
print(output.shape)
print(output[0].shape)
output = model.predict_on_batch(output)


print('Position of maximum probability: {}'.format(output[0].argmax()))
print('Maximum probability: {:.5f}'.format(max(output[0][0])))
print('Corresponding label: {}'.format(labels[output[0].argmax()]))

