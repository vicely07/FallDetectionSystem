'''
import numpy

train_d = numpy.load('train_data.npy')

train_l = numpy.load('train_label.npy')

test_d = numpy.load('test_data.npy')

test_l = numpy.load('test_label.npy')
import matplotlib.pyplot as plt

def plot_history(history):
    plt.plot(history.history['acc'], marker='.')
    plt.plot(history.history['val_acc'], marker='.')
    plt.title('model accuracy')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.grid()
    plt.legend(['acc', 'val_acc'], loc='lower right')
    plt.show()
    plt.savefig('model_accuracy.png')
    plt.close()

    plt.plot(history.history['loss'], marker='.')
    plt.plot(history.history['val_loss'], marker='.')
    plt.title('model loss')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.grid()
    plt.legend(['loss', 'val_loss'], loc='upper right')
    plt.show()
    plt.savefig('model_loss.png')
    plt.close()

def save_history(history):
    loss = history.history['loss']
    acc = history.history['acc']
    val_loss = history.history['val_loss']
    val_acc = history.history['val_acc']
    nb_epoch = len(acc)

    with open('result.txt', 'w') as fp:
        fp.write('epoch\tloss\tacc\tval_loss\tval_acc\n')
        for i in range(nb_epoch):
            fp.write('{}\t{}\t{}\t{}\t{}\n'.format(
                i, loss[i], acc[i], val_loss[i], val_acc[i]))
'''
#-------------------------------------------data handler--------------------------------------------------------------
import keras.backend as K

import numpy as np

def get_activations(model, inputs, print_shape_only=False, layer_name=None):



    # Documentation is available online on Github at the address below.



    # From: https://github.com/philipperemy/keras-visualize-activations



    print('----- activations -----')



    activations = []



    inp = model.input



    if layer_name is None:



        outputs = [layer.output for layer in model.layers]



    else:



        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs



    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions



    layer_outputs = [func([inputs, 1.])[0] for func in funcs]



    for layer_activations in layer_outputs:



        activations.append(layer_activations)



        if print_shape_only:



            print(layer_activations.shape)



        else:



            print(layer_activations)



    return activations











def get_data(n, input_dim, attention_column=1):



    """



    Data generation. x is purely random except that it's first value equals the target y.



    In practice, the network should learn that the target = x[attention_column].



    Therefore, most of its attention should be focused on the value addressed by attention_column.



    :param n: the number of samples to retrieve.



    :param input_dim: the number of dimensions of each element in the series.



    :param attention_column: the column linked to the target. Everything else is purely random.



    :return: x: model inputs, y: model targets



    """



    x = np.random.standard_normal(size=(n, input_dim))



    y = np.random.randint(low=0, high=2, size=(n, 1))



    x[:, attention_column] = y[:, 0]



    return x, y











def get_data_recurrent(n, time_steps, input_dim, attention_column=10):



    """



    Data generation. x is purely random except that it's first value equals the target y.



    In practice, the network should learn that the target = x[attention_column].



    Therefore, most of its attention should be focused on the value addressed by attention_column.



    :param n: the number of samples to retrieve.



    :param time_steps: the number of time steps of your series.



    :param input_dim: the number of dimensions of each element in the series.



    :param attention_column: the column linked to the target. Everything else is purely random.



    :return: x: model inputs, y: model targets


    """
    x = np.random.standard_normal(size=(n, time_steps, input_dim))
    y = np.random.randint(low=0, high=2, size=(n, 1))
    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))
    return x, y
   
#------------------------------------------lstm attention model-------------------------------------------------------

from keras.layers import TimeDistributed
from keras.layers import merge
from keras.layers.core import *
from keras.layers.recurrent import LSTM
from keras.models import *
#from attention_utils import get_activations, get_data_recurrent
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np

#INPUT_DIM = 512
#TIME_STEPS = 20

#class attLSTM(Layer):

def attention_3d_block(inputs):
    # inputs.shape = (batch_size, time_steps, input_dim)
    #input_dim = int(inputs.shape[2])
    a = Permute((2, 1))(inputs)
    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.
    a = Dense(20, activation='softmax')(a)
    '''
    if SINGLE_ATTENTION_VECTOR:
        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)
        a = RepeatVector(input_dim)(a)
    '''
    a_probs = Permute((2, 1), name='attention_vec')(a)
    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')
    return output_attention_mul

def model_attention_applied_before_lstm():
    inputs = Input(shape=(20, 512,))
    attention_mul = attention_3d_block(inputs)
    lstm_units = 24
    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)
    output = Dense(1, activation='sigmoid')(attention_mul)
    output = Dense(1, activation='softmax')(output)
    output = Dropout(0.5)(output)
    model = Model(input=[inputs], output=output)
    return model





#---------------------------------------------model---------------------------------------------------------------
from keras.models import Sequential
from keras.layers import Dense
from keras.layers.advanced_activations import LeakyReLU
from keras.losses import categorical_crossentropy
from keras.models import Sequential
from keras.optimizers import Adam
from keras.utils import np_utils
from keras.utils.vis_utils import plot_model
from sklearn.model_selection import train_test_split
from keras.utils.vis_utils import plot_model
from keras.layers.convolutional import Convolution3D, MaxPooling3D
# Define model
model = Sequential()
input_shape=(16, 112, 112, 3) # l, h, w, c


model.add(Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same', name='conv1', input_shape=input_shape))
model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1'))

# 2nd layer group

model.add(Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same', name='conv2'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool2'))

# 3rd layer group

model.add(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3a'))
model.add(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3b'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3'))

# 4th layer group

model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv4a'))
model.add(Convolution3D(512, 3, 3, 3, activation='relu',border_mode='same', name='conv4b'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4'))

# 5th layer group

model.add((Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5a')))
model.add((Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5b')))
model.add((MaxPooling3D(pool_size=(2, 1, 1), strides=(2, 1, 1), border_mode='valid', name='pool5')))

image_input = Input(shape=(1,7,7,512))
encoded_image = model(image_input)


#reshaped = tf.reshape(model.output_shape, [5])
#print(reshaped)
#from numpy import array
#data = array(model.output_shape)
#data = data.reshape((1, 5, 1))
#model.add(LSTM(24, input_shape=(5, 1)))
#model.add(Dense(1))

#attention lstm model
inputs_1, outputs = get_data_recurrent(300, 20, 512)

model.add(Lambda(model_attention_applied_before_lstm(), output_shape = model.output_shape))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#model.add(Dense(25, input_shape=(25,), activation='softmax'))
#model.add(Dropout(0.5))
#plot_model(model, show_shapes=True,to_file=os.path.join('D:/cognitive/sports/', 'model.png'))

print(model.summary())

'''
#train_d, train_l, test_d, test_l = train_test_split(X, Y, test_size=0.2, random_state=43)

history = model.fit(train_d, train_l, validation_data=(test_d, test_l), batch_size=130,epochs=100, verbose=1, shuffle=True)
plot_history(history)
save_history(history)
#model.evaluate(test_d, test_l, verbose=1)
model_json = model.to_json()
   
with open( 'ucf101_3dcnnmodel.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights( 'ucf101_3dcnnmodel.hd5')

loss, acc = model.evaluate(test_d, test_l, verbose=1)
print('Test loss:', loss)
print('Test accuracy:', acc)
'''
