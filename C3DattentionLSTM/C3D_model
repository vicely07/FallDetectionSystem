'''
import numpy

train_d = numpy.load('train_data.npy')

train_l = numpy.load('train_label.npy')

test_d = numpy.load('test_data.npy')

test_l = numpy.load('test_label.npy')
import matplotlib.pyplot as plt

def plot_history(history):
    plt.plot(history.history['acc'], marker='.')
    plt.plot(history.history['val_acc'], marker='.')
    plt.title('model accuracy')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.grid()
    plt.legend(['acc', 'val_acc'], loc='lower right')
    plt.show()
    plt.savefig('model_accuracy.png')
    plt.close()

    plt.plot(history.history['loss'], marker='.')
    plt.plot(history.history['val_loss'], marker='.')
    plt.title('model loss')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.grid()
    plt.legend(['loss', 'val_loss'], loc='upper right')
    plt.show()
    plt.savefig('model_loss.png')
    plt.close()

def save_history(history):
    loss = history.history['loss']
    acc = history.history['acc']
    val_loss = history.history['val_loss']
    val_acc = history.history['val_acc']
    nb_epoch = len(acc)

    with open('result.txt', 'w') as fp:
        fp.write('epoch\tloss\tacc\tval_loss\tval_acc\n')
        for i in range(nb_epoch):
            fp.write('{}\t{}\t{}\t{}\t{}\n'.format(
                i, loss[i], acc[i], val_loss[i], val_acc[i]))
'''
#------------------------------------------lstm attention model-------------------------------------------------------

from keras.layers import merge



from keras.layers.core import *



from keras.layers.recurrent import LSTM



from keras.models import *







INPUT_DIM = 3



TIME_STEPS = 20



# if True, the attention vector is shared across the input_dimensions where the attention is applied.



SINGLE_ATTENTION_VECTOR = False



APPLY_ATTENTION_BEFORE_LSTM = False











def attention_3d_block(inputs):



    # inputs.shape = (batch_size, time_steps, input_dim)



    input_dim = int(inputs.shape[2])



    a = Permute((2, 1))(inputs)



    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.



    a = Dense(TIME_STEPS, activation='softmax')(a)



    if SINGLE_ATTENTION_VECTOR:



        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)



        a = RepeatVector(input_dim)(a)



    a_probs = Permute((2, 1), name='attention_vec')(a)



    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')



    return output_attention_mul





def model_attention_applied_before_lstm():



    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))

    

    attention_mul = attention_3d_block(inputs)



    lstm_units = 24



    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)



    output = Dense(1, activation='sigmoid')(attention_mul)



    model = Model(input=[inputs], output=output)



    return model





#---------------------------------------------model---------------------------------------------------------------
from keras.models import Sequential
from keras.layers import Dense
from keras.layers.advanced_activations import LeakyReLU
from keras.losses import categorical_crossentropy
from keras.models import Sequential
from keras.optimizers import Adam
from keras.utils import np_utils
from keras.utils.vis_utils import plot_model
from sklearn.model_selection import train_test_split
from keras.utils.vis_utils import plot_model
from keras.layers.convolutional import Convolution3D, MaxPooling3D
# Define model
model = Sequential()

input_shape=(16, 112, 112, 3) # l, h, w, c


model.add(Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same', name='conv1', input_shape=input_shape))
model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1'))

# 2nd layer group

model.add(Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same', name='conv2'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool2'))

# 3rd layer group

model.add(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3a'))
model.add(Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3b'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3'))

# 4th layer group

model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv4a'))
model.add(Convolution3D(512, 3, 3, 3, activation='relu',border_mode='same', name='conv4b'))
model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4'))

# 5th layer group

model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5a'))
model.add(Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv5b'))
model.add(MaxPooling3D(pool_size=(2, 1, 1), strides=(2, 1, 1), border_mode='valid', name='pool5'))

#attention LSTM model
model.add(model_attention_applied_before_lstm())
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.add(Dropout(0.5))
model.add(Dense(11, activation='softmax'))


print(model.summary())
#plot_model(model, show_shapes=True,to_file=os.path.join('D:/cognitive/sports/', 'model.png'))

'''
#train_d, train_l, test_d, test_l = train_test_split(X, Y, test_size=0.2, random_state=43)

history = model.fit(train_d, train_l, validation_data=(test_d, test_l), batch_size=130,epochs=100, verbose=1, shuffle=True)
plot_history(history)
save_history(history)
#model.evaluate(test_d, test_l, verbose=1)
model_json = model.to_json()
   
with open( 'ucf101_3dcnnmodel.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights( 'ucf101_3dcnnmodel.hd5')

loss, acc = model.evaluate(test_d, test_l, verbose=1)
print('Test loss:', loss)
print('Test accuracy:', acc)
'''
